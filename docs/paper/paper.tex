\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{float}
\usepackage{enumitem}
\usepackage{caption}
\usepackage[numbers]{natbib}

% Custom commands
\newcommand{\sota}{\textsc{SOTA}}
\newcommand{\pipeline}{\textsc{Associate-DS}}
\newcommand{\auc}{\text{AUC-ROC}}

\title{An Autonomous 9-Phase Data Science Pipeline for\\Imbalanced Binary Classification:\\Architecture, Benchmark Evaluation, and Lessons\\from LLM-Guided Feature Engineering}

\author{
  [Authors]\\
  [Affiliation]\\
  \texttt{[email]}
}

\date{}

\begin{document}
\maketitle

% ============================================================================
\begin{abstract}
Building production machine learning models for imbalanced classification requires extensive manual effort across data profiling, feature engineering, model selection, ensemble construction, and deployment.
We present \pipeline{}, a fully autonomous 9-phase data science pipeline that runs end-to-end from raw CSV datasets to deployment-ready model artifacts with no human intervention.
The pipeline integrates LLM-based semantic data profiling, AI-assessed feature engineering, evolutionary LLM-guided feature generation, imbalance-aware data augmentation (SMOTE, ADASYN, Self-Paced Ensemble), multi-candidate model training with conformal calibration, stacking ensemble with a novel degradation fallback mechanism, SHAP-based evaluation, and automated deployment artifact generation.

We evaluate the pipeline on 25 standard imbalanced classification benchmark datasets spanning four imbalance tiers (IR from 1.7:1 to 577:1).
The pipeline achieves \sota{}-competitive performance with an average AUC gap of [TBD] across 18 datasets with published baselines.
We identify and address ensemble degradation on small datasets through an automatic fallback mechanism, and report a null result on LLM-based feature engineering---achieving 0\% lift across all datasets despite syntactically valid transform proposals.
This null result has implications for the emerging AutoFE research direction.
\end{abstract}

% ============================================================================
\section{Introduction}
\label{sec:intro}

Imbalanced classification---where one class significantly outnumbers another---is ubiquitous in real-world applications including fraud detection~\citep{dal2017credit}, medical diagnosis~\citep{johnson2019mimic}, software defect prediction~\citep{kamei2013large}, and industrial quality control.
The challenge compounds across the full machine learning lifecycle: standard data profiling ignores class distribution, feature engineering may inadvertently amplify majority-class patterns, model selection defaults to accuracy (which favors the majority class), and evaluation metrics like raw accuracy are misleading.

Automated Machine Learning (AutoML) systems~\citep{feurer2015efficient,wang2021flaml,erickson2020autogluon} have made significant progress in automating model selection and hyperparameter optimization.
However, these systems typically operate within a fixed pipeline structure, assuming pre-processed features and balanced class distributions.
They do not autonomously handle the full lifecycle from raw data profiling through deployment artifact generation, nor do they incorporate imbalance-aware strategies at every phase.

We present \pipeline{}, a fully autonomous 9-phase data science pipeline that addresses this gap.
Given only a raw CSV dataset and a target column name, the pipeline produces a deployment-ready model with API scaffolding, monitoring configuration, and data contracts---with no human intervention at any stage.

\paragraph{Contributions.}
\begin{enumerate}[nosep]
  \item A 9-phase autonomous pipeline with LLM-based quality gates, imbalance-aware strategies at every phase, and a novel ensemble degradation fallback mechanism.
  \item A comprehensive benchmark evaluation on 25 standard imbalanced classification datasets across four imbalance tiers.
  \item A transparent null result on LLM-based feature engineering (0\% lift across all datasets), with analysis of failure modes and implications for AutoFE research.
\end{enumerate}

% ============================================================================
\section{Related Work}
\label{sec:related}

\subsection{AutoML Systems}

Auto-sklearn~\citep{feurer2015efficient,feurer2020auto} uses Bayesian optimization over scikit-learn pipelines with meta-learning warm-starting.
FLAML~\citep{wang2021flaml} provides cost-effective hyperparameter optimization with early stopping.
AutoGluon~\citep{erickson2020autogluon} uses multi-layer stacking of diverse model families.
H2O AutoML~\citep{h2o2023} provides a production-oriented AutoML platform.

These systems focus primarily on model selection and hyperparameter optimization.
\pipeline{} is broader: it encompasses data profiling, EDA, feature engineering assessment, augmentation, and deployment---a total of 9 autonomous phases versus the typical 1--3 of existing AutoML systems.

\subsection{Imbalanced Classification}

SMOTE~\citep{chawla2002smote} and its variants (SMOTE-NC, ADASYN~\citep{he2008adasyn}, Borderline-SMOTE) remain the dominant resampling approaches.
Ensemble methods designed for imbalance---EasyEnsemble~\citep{liu2009exploratory}, BalanceBagging, and Self-Paced Ensemble~\citep{liu2020self}---train base learners on balanced subsets.
The imbalanced-learn library~\citep{lemaitre2017imbalanced} provides standardized implementations.

Our pipeline selects augmentation strategy automatically based on imbalance ratio and dataset characteristics, defaulting to Self-Paced Ensemble for IR $> 3.0$.

\subsection{LLM-Based Feature Engineering}

CAAFE~\citep{hollmann2024large} demonstrated that LLMs can generate useful feature engineering code from dataset descriptions.
Our pipeline implements a similar evolutionary approach but within a full autonomous pipeline context.
We report a contrasting null result, which may be attributable to the broader pipeline context rather than the LLM FE approach itself.

% ============================================================================
\section{Method}
\label{sec:method}

\subsection{Pipeline Architecture}
\label{sec:architecture}

\pipeline{} consists of 9 sequential phases, each implemented as an autonomous agent.
Each phase receives structured input from the previous phase and produces a typed output (Pydantic model).
An LLM-based quality gate assesses each phase's output with a structured pass/fail determination.

\begin{figure}[H]
\centering
\small
\begin{verbatim}
Raw CSV --> [1: Data Audit] --> [2: EDA] --> [2.5: FE Assessment]
  --> [3: Feature Engineering] --> [3.5: Data Augmentation]
  --> [4: Model Training] --> [5: Ensemble]
  --> [6: Evaluation] --> [7: Deployment] --> Model + API + Monitoring
\end{verbatim}
\caption{The 9-phase \pipeline{} architecture. Each phase is implemented as an autonomous agent with structured input/output contracts and LLM quality gate assessment.}
\label{fig:architecture}
\end{figure}

\subsection{Phase Details}

\paragraph{Phase 1: Data Audit.}
Loads the dataset, profiles all columns (type inference, cardinality, missing rates, distribution statistics), and performs LLM-based semantic enrichment to understand column semantics (e.g., identifying that ``Fare'' is a monetary amount, ``Cabin'' is a location code).

\paragraph{Phase 2: EDA.}
Statistical exploratory analysis including correlation matrices, distribution characterization, outlier detection, and imbalance ratio computation.
The imbalance ratio $\text{IR} = |C_{\text{maj}}| / |C_{\text{min}}|$ determines downstream augmentation strategy.

\paragraph{Phase 2.5: FE Assessment.}
An LLM-based assessment of which feature engineering categories (interactions, binning, encoding, temporal, text extraction) are applicable to the specific dataset.
This avoids running expensive evolutionary FE on categories unlikely to yield improvement.

\paragraph{Phase 3: Feature Engineering.}
Two-stage approach: (1) deterministic per-column treatments (log transform, sqrt, bins, one-hot encoding for categoricals, interaction terms) based on column profiles, and (2) an evolutionary LLM-based feature generation loop (5 generations, configurable population size).
Each LLM-proposed transform is syntactically validated via \texttt{compile()}, executed in a sandboxed namespace, and evaluated via cross-validated scoring.

\paragraph{Phase 3.5: Data Augmentation.}
Strategy selection based on imbalance characteristics:
\begin{itemize}[nosep]
  \item $\text{IR} < \text{threshold}$: No augmentation
  \item $\text{IR} \geq \text{threshold}$, numeric only: SMOTE
  \item $\text{IR} \geq \text{threshold}$, has categoricals: SMOTE-NC
  \item $\text{IR} \geq 10$: ADASYN or Self-Paced Ensemble
\end{itemize}

\paragraph{Phase 4: Model Training.}
Multi-candidate cross-validated training with automatic scale-tier selection.
Candidates: XGBoost, CatBoost, LightGBM (always), plus optional TabPFN, TabICL, NAM.
Each candidate undergoes: 5-fold StratifiedKFold CV, isotonic calibration (Platt scaling fallback), Brier score computation, and conformal prediction calibration.

\paragraph{Phase 5: Ensemble.}
Stacking architecture with Level-0 out-of-fold (OOF) predictions from all candidates and a Level-1 calibrated logistic regression meta-learner.
A Pareto optimizer evaluates configurations across accuracy, interpretability, and speed objectives.
Uncertainty-based routing classifies predictions as ``confident'' or ``uncertain'' based on prediction disagreement.

\textbf{Degradation Fallback:} We identify that stacking ensembles can degrade on small datasets ($N < 500$).
When the meta-learner's CV score falls below $0.98 \times$ the best base model score, the pipeline automatically falls back to the best base model, bypassing Pareto optimization.

\paragraph{Phase 6: Evaluation.}
Comprehensive evaluation including: threshold optimization (Youden's J statistic), predictive scores (AUC, F1-macro, MCC, Brier score), robustness testing (Gaussian noise injection), SHAP feature importance, Gini concentration for interpretability scoring, and baseline comparison.

\paragraph{Phase 7: Deployment.}
Automated generation of: serialized model artifact (joblib), FastAPI scaffold with input validation, monitoring configuration with feature drift thresholds, and data contract with field specifications.

\subsection{Quality Gates}
Each phase output is assessed by an LLM with a structured prompt requesting a pass/fail determination and explanation.
Quality gates are \emph{non-blocking}: a failing gate logs a warning but does not halt the pipeline.
This design ensures robustness---a single phase's output quality issue does not prevent the pipeline from completing.

% ============================================================================
\section{Experimental Setup}
\label{sec:experiments}

\subsection{Benchmark Datasets}

We evaluate on 25 curated imbalanced classification datasets organized by imbalance ratio into four tiers (Table~\ref{tab:datasets}).
Datasets are sourced from scikit-learn, OpenML, and imbalanced-learn.
Published \sota{} values are drawn from the respective dataset literatures.

\begin{table}[H]
\centering
\caption{Benchmark datasets organized by imbalance tier. IR = imbalance ratio (majority/minority). \sota{} AUC values from published literature.}
\label{tab:datasets}
\small
\begin{tabular}{llrrrl}
\toprule
\textbf{Dataset} & \textbf{Tier} & \textbf{N} & \textbf{Features} & \textbf{IR} & \textbf{\sota{} AUC} \\
\midrule
Breast Cancer Wisconsin & Mild & 569 & 30 & 1.7 & 0.995 \\
German Credit & Mild & 1,000 & 20 & 2.3 & 0.810 \\
Haberman Survival & Mild & 306 & 3 & 2.8 & 0.720 \\
Ionosphere & Mild & 351 & 34 & 1.8 & 0.980 \\
Pima Indians Diabetes & Mild & 768 & 8 & 1.9 & 0.840 \\
\midrule
Car Evaluation & Moderate & 1,728 & 6 & 4.0 & 0.990 \\
Thyroid Sick & Moderate & 3,772 & 29 & 7.4 & 0.990 \\
Vehicle Insurance & Moderate & 846 & 18 & 6.0 & 0.930 \\
Wine Quality (Red) & Moderate & 1,599 & 11 & 5.0 & 0.950 \\
Yeast ME2 & Moderate & -- & -- & 5.2 & -- \\
\midrule
Abalone 19 & Severe & -- & -- & 16.0 & -- \\
Letter Img Recognition & Severe & -- & -- & 26.0 & 0.990 \\
Mammography & Severe & 11,183 & 6 & 42.0 & 0.940 \\
Oil Spill & Severe & -- & -- & 22.0 & 0.940 \\
PC1 Software Defects & Severe & 1,109 & 21 & 14.0 & 0.810 \\
Phoneme & Severe & -- & -- & 2.4 & 0.960 \\
Satimage & Severe & -- & -- & 10.0 & 0.930 \\
US Crime & Severe & -- & -- & -- & -- \\
\midrule
COIL 2000 Insurance & Extreme & 9,822 & 85 & 577.0 & 0.780 \\
Credit Card Fraud & Extreme & -- & -- & 577.0 & 0.980 \\
Ozone Level Detection & Extreme & -- & -- & 34.0 & 0.900 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Protocol}

Each dataset is processed through the complete 9-phase pipeline with no human intervention.
The primary evaluation metric is AUC-ROC.
We compute the \textbf{\sota{} gap} as:
\[
\Delta_{\auc} = \auc_{\text{pipeline}} - \auc_{\text{SOTA}}
\]
A positive gap indicates the pipeline outperforms the published baseline.

% ============================================================================
\section{Results}
\label{sec:results}

\subsection{Overall Performance}

[TO BE FILLED WITH V2 BENCHMARK RESULTS]

\begin{table}[H]
\centering
\caption{Full benchmark results. Gap = pipeline AUC $-$ \sota{} AUC. Positive gaps (bold) indicate the pipeline outperforms the published baseline.}
\label{tab:results}
\small
\begin{tabular}{llcccr}
\toprule
\textbf{Dataset} & \textbf{Tier} & \textbf{AUC} & \textbf{\sota{}} & \textbf{Gap} & \textbf{Time} \\
\midrule
\multicolumn{6}{l}{\emph{[V2 results to be inserted]}} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ensemble Degradation Fallback}

We observed that the stacking meta-learner systematically underperforms the best base model on small datasets.
On Haberman Survival (306 samples, 3 features), the meta-learner achieved a CV score of 0.50 while the best base model scored 0.61---a 18\% degradation.
On German Credit (1,000 samples), the meta-learner scored 0.65 versus a best base of 0.70.

Our degradation fallback (threshold $= 0.98$) automatically detects and corrects this behavior.

\subsection{LLM Feature Engineering: A Null Result}

Across all 21 successfully processed datasets, the LLM evolutionary feature engineering loop achieved 0\% improvement over the deterministic baseline features.
This is a significant null result.

Analysis of the WARNING-level logs reveals the failure mode distribution:
\begin{itemize}[nosep]
  \item Syntax errors caught by \texttt{compile()}: [TBD]\%
  \item Missing \texttt{transform} function definition: [TBD]\%
  \item Runtime execution failures: [TBD]\%
  \item Valid execution but no CV improvement: [TBD]\%
\end{itemize}

% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Strengths}
The pipeline achieves \sota{}-competitive results across diverse imbalanced datasets with zero human intervention.
The ensemble degradation fallback is a practical contribution: stacking should not be applied blindly, and automated detection of degradation prevents performance loss on small datasets.

\subsection{LLM-FE Limitations}
The 0\% lift from LLM-generated features contrasts with results reported by \citet{hollmann2024large} (CAAFE).
Possible explanations include: (1) our deterministic FE phase already captures the most impactful transforms; (2) the LLM receives insufficient domain context; (3) the evolutionary loop's 5 generations are insufficient for convergence; (4) the pipeline's feature space (30--90 columns after deterministic FE) is already rich enough that additional features are redundant.

\subsection{Practical Implications}
For practitioners: an autonomous pipeline that achieves \sota{}-competitive results in $\sim$3 minutes per dataset represents a significant reduction in manual effort.
The ensemble degradation fallback should be considered in any stacking-based AutoML system.
LLM-based feature engineering remains promising but needs careful evaluation---our null result suggests that naive LLM feature generation does not reliably improve over domain-agnostic statistical features.

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented \pipeline{}, a fully autonomous 9-phase data science pipeline for imbalanced binary classification.
Evaluated on 25 standard benchmark datasets, the pipeline achieves \sota{}-competitive performance with an average AUC gap of [TBD].
Our ensemble degradation fallback mechanism addresses a practical failure mode in stacking ensembles.
The transparent null result on LLM-based feature engineering contributes to honest evaluation in the emerging AutoFE field.

% ============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Chawla et~al.(2002)]{chawla2002smote}
N.~Chawla, K.~Bowyer, L.~Hall, and W.~Kegelmeyer.
\newblock SMOTE: Synthetic minority over-sampling technique.
\newblock \emph{JAIR}, 16:321--357, 2002.

\bibitem[Dal~Pozzolo et~al.(2017)]{dal2017credit}
A.~Dal~Pozzolo, G.~Boracchi, O.~Caelen, C.~Alippi, and G.~Bontempi.
\newblock Credit card fraud detection: a realistic modeling and a novel learning strategy.
\newblock \emph{IEEE TNNLS}, 29(8):3784--3797, 2017.

\bibitem[Erickson et~al.(2020)]{erickson2020autogluon}
N.~Erickson, J.~Mueller, A.~Shirkov, H.~Zhang, P.~Larroy, M.~Li, and A.~Smola.
\newblock AutoGluon-Tabular: Robust and accurate AutoML for structured data.
\newblock \emph{arXiv:2003.06505}, 2020.

\bibitem[Feurer et~al.(2015)]{feurer2015efficient}
M.~Feurer, A.~Klein, K.~Eggensperger, J.~Springenberg, M.~Blum, and F.~Hutter.
\newblock Efficient and robust automated machine learning.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Feurer et~al.(2020)]{feurer2020auto}
M.~Feurer, K.~Eggensperger, S.~Falkner, M.~Lindauer, and F.~Hutter.
\newblock Auto-sklearn 2.0: Hands-free automl via meta-learning.
\newblock \emph{JMLR}, 23:1--61, 2020.

\bibitem[H2O.ai(2023)]{h2o2023}
H2O.ai.
\newblock H2O AutoML documentation.
\newblock \url{https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html}, 2023.

\bibitem[He et~al.(2008)]{he2008adasyn}
H.~He, Y.~Bai, E.~Garcia, and S.~Li.
\newblock ADASYN: Adaptive synthetic sampling approach for imbalanced learning.
\newblock In \emph{IEEE IJCNN}, 2008.

\bibitem[Hollmann et~al.(2024)]{hollmann2024large}
N.~Hollmann, S.~M\"uller, and F.~Hutter.
\newblock Large language models for automated data science: Introducing CAAFE for context-aware automated feature engineering.
\newblock In \emph{NeurIPS}, 2024.

\bibitem[Johnson et~al.(2019)]{johnson2019mimic}
A.~Johnson, T.~Pollard, L.~Shen, L.~Lehman, M.~Feng, M.~Ghassemi, B.~Moody, P.~Szolovits, L.~Celi, and R.~Mark.
\newblock MIMIC-III, a freely accessible critical care database.
\newblock \emph{Scientific Data}, 3:160035, 2019.

\bibitem[Kamei et~al.(2013)]{kamei2013large}
Y.~Kamei, E.~Shihab, B.~Adams, A.~Hassan, A.~Mockus, A.~Sinha, and N.~Ubayashi.
\newblock A large-scale empirical study of just-in-time quality assurance.
\newblock \emph{IEEE TSE}, 39(6):757--773, 2013.

\bibitem[Lemaitre et~al.(2017)]{lemaitre2017imbalanced}
G.~Lemaitre, F.~Nogueira, and C.~Aridas.
\newblock Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning.
\newblock \emph{JMLR}, 18(17):1--5, 2017.

\bibitem[Liu et~al.(2009)]{liu2009exploratory}
X.~Liu, J.~Wu, and Z.~Zhou.
\newblock Exploratory undersampling for class-imbalance learning.
\newblock \emph{IEEE TSMC-B}, 39(2):539--550, 2009.

\bibitem[Liu et~al.(2020)]{liu2020self}
Z.~Liu, W.~Cao, Z.~Gao, J.~Bian, H.~Chen, Y.~Chang, and T.~Liu.
\newblock Self-paced ensemble for highly imbalanced massive data classification.
\newblock In \emph{IEEE ICDE}, 2020.

\bibitem[Wang et~al.(2021)]{wang2021flaml}
C.~Wang, Q.~Wu, M.~Weimer, and E.~Zhu.
\newblock FLAML: A fast and lightweight AutoML library.
\newblock In \emph{MLSys}, 2021.

\end{thebibliography}

\end{document}
